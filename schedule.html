<!DOCTYPE HTML>
<html>
	<head>
		<title>Welcome to LA4IRA@ROMAN’23!</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
	</head>
	
	<style>
    table {
	border: 1px;
	border-collapse: separate;
	font: inherit;
	border-spacing: 5px;
    }
    tr, td {
	border: 1px solid black;
        border-collapse: separate;
	border-spacing: 5px;
		font: inherit;
    }
    tr, td {
        padding: 5px;
    }
	td{
		vertical-align:middle;
	}
	tr > th > h2{
		font-size: 100% !important;
	}
    </style>
	
	<body class="is-preload">
		<div id="page-wrapper">

			<!-- Header -->
				<div id="header">
					<!-- Logo -->
<!--						<img src="images/isail_logo.png" alt="" /><span><h2>Welcome to <a href="index.html" id="logo">iSAIL</a> Lab !</h2></span>-->

					<!-- Nav -->
						<nav id="nav">
							<ul>
								<li><a href="index.html">Home</a></li>
								<li><a href="cfp.html">Call For Paper</a></li>
								<li class="current"><a href="schedule.html">Schedule</a></li>
								<li><a href="keynote.html">Keynote & Invited</a></li>
								<li><a href="publications.html">Accepted Papers</a></li>
								<li><a href="organization.html">Organization</a></li>
								<!--<li><a href="history.html">History</a></li>-->
							</ul>
						</nav>

				</div>
			<!-- Banner -->
				<section id="banner">
					<header>
						<h2>Welcome to Learning by Asking for Intelligent Robots and Agents (LA4IRA@ROMAN’23)!</h2>
						<!--<a href="#" class="button">Learn More</a>-->
					</header>
				</section>


				<section class="wrapper style1">
					<div class="container">
						<div class="row">
							<div class="col-lg-12 text-center">
								<h2 class="section-heading">Schedule: August 28th, 2023 (KST) </h2>
							</div>
						</div>


						<center>
						<table class="table">
						<colgroup>
							<col width="12%" />
							<col width="63%" />
							<col width="25%" />
						</colgroup>


						<thead>
							<tr bgcolor="#f7f1df">
							<th><h2 font-size="75%">Time</h2></th>
							<th><h2 font-size="75%">Title</h2></th>
							<th><h2 font-size="75%">Speaker</h2></th>
							</tr>
						</thead>
						
						
						<tbody>
						
												
							<tr>
								<td>09:00-09:05am</td>
								<td>Session: Opening Remarks</td>
								<td style="text-align:center;vertical-align:middle">
									<div id="images">
                                                                        </div>
									<div id="images">
                                                                        </div>
									<div id="images">
                                                                        </div>
									
								</td>
								
							</tr>
							<tr>
								<td>09:05-09:35am</td>
								<td>
									<em>Keynote talk: "Embodied AI: Machine Learning to Learning Machines"</em>
									</br>
									</br>
									<p>
										Abstract: Machine learning (including deep learning) has changed the paradigm of AI from rule-based “manual” programming to data-driven “automatic” programming. However, the current paradigm of machine learning requires some external system that provides them with data, making their scalability limited. Here we argue that the learner can feed itself the data autonomously if it is embodied, i.e. equipped with sensors and actuators. With the perception-action cycle the embodied AI can continually learn to solve problems in a self-teaching way by doing new actions, observing their outcomes, and correcting their own predictions like the humans and animals do. In this talk, I will show some of our studies in this direction of “(embodied) learning machine” research and discuss its implications for achieving truly human-level general AI.
									</p>
								</td>
								<td style="text-align:center;vertical-align:middle">
									<div id="images">
										<a href="http://bi.snu.ac.kr/~btzhang">
											<div class="caption">Byoung-Tak Zhang</div>
											<div class="caption">Seoul National University</div>
										</a>
									</div>
								</td>
							</tr>
							<tr>
								<td>09:35-10:05am</td>
								<td>
									<em>Invited Talk 1: "Following Instructions and Asking Questions"</em>
									</br>
									</br>
									<p>
										Abstract: As we move towards the creation of embodied agents that understand natural language, several new challenges and complexities arise for grounding (e.g. complex state-spaces), planning (e.g. long horizons), and social interaction (e.g. asking for help or clarifications).  In this talk, I'll discuss several recent results both on improvements to embodied instruction following within ALFRED and initial steps towards building agents that ask questions or model theory-of-mind.
									</p>
								</td>
								<td style="text-align:center;vertical-align:middle">
									<div id="images">
										<a href="https://yonatanbisk.com">
											<div class="caption">Yonatan Bisk</div>
											<div class="caption">Carnegie Mellon University</div>
										</a>
									</div>								
								</td>
							</tr>
							<tr>
                                                                <td>10:05-10:35am</td>
                                                                <td>
                                                                        <em>Invited Talk 2: "Scaling Robot Learning by Understanding Videos"</em>
                                                                        </br>
                                                                        </br>
                                                                        <p>
                                                                                Abstract: True gains of machine learning in AI sub-fields such as computer vision and natural language processing have come about from the use of large-scale diverse datasets for learning. In this talk, I will discuss if and how we can leverage large-scale diverse data in the form of egocentric videos (first-person videos of humans conducting different tasks) to similarly scale up policy learning for robots. I will discuss the challenges this presents, and some of our initial efforts towards tackling them. In particular, I will describe techniques to acquire low-level visuomotor subroutines, high-level value functions, and an interactive understanding of objects from in-the-wild egocentric videos.
                                                                        </p>
                                                                </td>
                                                                <td style="text-align:center;vertical-align:middle">
                                                                        <div id="images">
                                                                                <a href="http://saurabhg.web.illinois.edu/">
                                                                                        <div class="caption">Saurabh Gupta</div>
                                                                                        <div class="caption">University of Illinois Urbana-Champaign</div>
                                                                                </a>
                                                                        </div>
                                                                </td>
                                                        </tr>

							<tr>
								<td>10:35-10:45am</td>
								<td>10-min Coffee Break/Social Networking</td>
								<td style="text-align:center"></td>
							</tr>

							<tr>
								<td>10:45-11:55am</td>
								
								<td>
									<p>3-min teaser talks for poster presentations / Poster Session <div class="myline"></div></p>
									
									<p>Paper 1: ECLAIR: Event-Cognizant Language Interaction Embodied Robots
									</br> 
									<em>Jinyeon Kim, Byeonghwi Kim, Cheolhong Min, Yuyeong Kim, Taewoong Kim and Jonghyun Choi</em>
									<div class="myline"></div> </p>
	
									<p>Paper 2: GVCCI: Lifelong Learning of Visual Grounding for Language-Guided Robotic Manipulation 
									</br>
									<em>Junghyun Kim, Gi-Cheon Kang, Jaein Kim, Suyeon Shin and Byoung-Tak Zhang</em>
									<div class="myline"></div> </p>

									<p>Paper 3: Continual Fine-tuning using Linearized Deep Neural Networks 
									</br>
                                                                        <em>Hyounguk Shon and Junmo Kim</em>
									<div class="myline"></div> </p>

									<p>Paper 4: The Dialog Must Go On: Improving Visual Dialog via Generative Self-Training 
									</br>
                                                                        <em>Gi-Cheon Kang, Sungdong Kim, Jin-Hwa Kim, Donghyun Kwak and Byoung-Tak Zhang</em>
									<div class="myline"></div> </p>

									<p>Paper 5: Towards Robust Robot Perception: Enhancing Offline Reinforcement Learning with Adversarial Attacks and Defenses 
									</br>
                                                                        <em>Thanh Nguyen, Tung Luu and Chang Dong Yoo</em>
									<div class="myline"></div> </p>

									<p>Paper 6: CLIP is Also a Good Teacher in the Presence of Noisy Labels 
									</br>
                                                                        <em>Yeonguk Yu, Minhwan Ko and Kyoobin Lee</em>
									<div class="myline"></div> </p>

									<p>Paper 7: Leveraging Class-agnostic Object Proposals for Open-world Object Detection 
									</br>
                                                                        <em>Assefa Seyoum Wahd, Minsu Jang and Seung-Ik Lee</em>
									<div class="myline"></div> </p>

									<p>Paper 8: A Dataset Design for Question Generation Based on Human Cognition 
									</br>
                                                                        <em>Minjung Shin, Minsu Chang, Miyoung Cho and Jeh-Kwang Ryu</em>
									<div class="myline"></div> </p>

									<p>Paper 9: Reducing Object Hallucination for Image Captioning using Large Vision-Language Models with Reinforcement Learning 
									</br>
                                                                        <em>Hee Suk Yoon, Eunseop Yoon and Chang D. Yoo</em>
									<div class="myline"></div> </p>

									<p>Paper 10: A Data Set for Clarifying Ambiguous Questions with Intermediate Questions in Visual Question Answering 
									</br>
                                                                        <em>Gyu-Min Park and Seong-Bae Park</em>
									<div class="myline"></div> </p>

									<p>Paper 11: Spatio-Temporal Graph Random Walks to Understand Long Video Semantic Structure 
									</br>
                                                                        <em>Eun-Sol Kim, Hoyoung Yoon, Minseo Kim and Kyuyoung Lee</em>
									<div class="myline"></div> </p>
									
								</td>
								<td style="text-align:center"></td>
							</tr>

						<tr>
							<td>11:55-12:00pm</td>
							<td>Closing remarks</td>
							<td style="text-align:center;vertical-align:middle">
								<div id="images">
                                                                </div>
                                                                <div id="images">
                                                                </div>
								<div id="images">
                                                                </div>
							</td>
						</tr>


							
						</tbody>
						</table>
						</center>


					
					<!-- add three lines to debug: -->
					</section>
				</article>
			  </div>


			<!-- Footer -->
				<div id="footer">
					<div class="container">
						<div class="row">

							<section class="col-6 col-12-narrower">
								<h3>Get In Touch</h3>
								<!-- <style>
									table, tr, td {
										border: 1px solid rgb(0, 0, 0);
										border-collapse: separate;
										border-spacing: 5px;
										font: inherit;
									}
									tr, td {
										padding: 5px;
									}
									td{
										vertical-align:middle;
									}
									</style> -->
								<table style="width: 90%;border: 0px solid;border-spacing: 0px">
									<tbody>
										<tr class="trow">
											<td style="border: 0px;padding: 0px"><i class="fa fa-envelope" style="font-size: 20px;"></i> <a href="mailto:la4ira.workshop@gmail.com">la4ira.workshop@gmail.com</a></td>
										</tr>
										<!-- <tr class="trow"></tr>
											<td style="border: 0px;padding: 0px"><i class="fa fa-envelope" style="font-size: 20px;"></i> <a href="mailto:second@mail">second@mail">second@mail</a></td>
										</tr> -->
									</tbody>
								</table>
							</section>
						</div>
					</div>

					<!-- Icons -->
						<!-- <ul class="icons">
							<li><a href="#" class="icon brands fa-twitter"><span class="label">Twitter</span></a></li>
							<li><a href="#" class="icon brands fa-facebook-f"><span class="label">Facebook</span></a></li>
							<li><a href="#" class="icon brands fa-github"><span class="label">GitHub</span></a></li>
							<li><a href="#" class="icon brands fa-linkedin-in"><span class="label">LinkedIn</span></a></li>
							<li><a href="#" class="icon brands fa-google-plus-g"><span class="label">Google+</span></a></li>
						</ul> -->

					<!-- Copyright -->
						<div class="copyright">
							<ul class="menu">
								<li>&copy; LA4IRA. All rights reserved</li><li>Design: <a href="http://html5up.net">HTML5 UP</a></li>
							</ul>
						</div>

				</div>

		</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.dropotron.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>
